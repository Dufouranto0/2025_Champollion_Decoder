{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from omegaconf import OmegaConf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning import LightningDataModule\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: CINGULATE_left\n",
      "Numpy path: /neurospin/dico/data/deep_folding/current/datasets/UkBioBank40/crops/2mm/CINGULATE./mask/Lskeleton.npy\n",
      "Subjects path: /neurospin/dico/data/deep_folding/current/datasets/UkBioBank40/crops/2mm/CINGULATE./mask/Lskeleton_subject.csv\n",
      "subject column name: Subject\n",
      "Input size: (1, 18, 41, 38)\n"
     ]
    }
   ],
   "source": [
    "# Load decoder config\n",
    "decoder_cfg = OmegaConf.load(\"../configs/config.yaml\")\n",
    "\n",
    "# Load encoder's training config\n",
    "encoder_config_path = os.path.join(\n",
    "    decoder_cfg.model_to_decode_path,\n",
    "    \".hydra\",\n",
    "    \"config.yaml\"\n",
    ")\n",
    "encoder_cfg = OmegaConf.load(encoder_config_path)\n",
    "\n",
    "# Override dataset_folder which conains a mistake\n",
    "encoder_cfg[\"dataset_folder\"] = decoder_cfg[\"dataset_folder\"]\n",
    "\n",
    "# Only resolve the dataset part to avoid errors in unrelated keys\n",
    "region = list(encoder_cfg.dataset.keys())[0]\n",
    "print(\"Region:\", region)\n",
    "\n",
    "dataset_info = OmegaConf.to_container(encoder_cfg.dataset[region], resolve=True)\n",
    "\n",
    "# Access relevant dataset info\n",
    "\n",
    "print(\"Numpy path:\", dataset_info['numpy_all'])\n",
    "print(\"Subjects path:\", dataset_info['subjects_all'])\n",
    "print(\"subject column name:\", dataset_info['subject_column_name'])\n",
    "print(\"Input size:\", dataset_info['input_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CINGULATE_left (42433, 18, 41, 38, 1) 42433\n"
     ]
    }
   ],
   "source": [
    "skels = np.load(dataset_info['numpy_all'])\n",
    "list_sub = pd.read_csv(dataset_info['subjects_all'])\n",
    "print(region, skels.shape, len(list_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath = os.path.join(decoder_cfg[\"model_to_decode_path\"])\n",
    "train_path = os.path.join(rootpath, config[\"train_csv\"])\n",
    "val_test_path = os.path.join(rootpath, config[\"val_test_csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 38190\n",
      "Validation set size: 2092\n",
      "Test set size: 2151\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(train_path)\n",
    "a = pd.read_csv(val_test_path)\n",
    "a['IID'] = a['ID'].apply(lambda x : int(x[4:]))\n",
    "val_data = a[a['IID']%2 ==0].drop('IID', axis=1)\n",
    "test_data = a[a['IID']%2 ==1].drop('IID', axis=1)\n",
    "\n",
    "#train_data = train_data.drop('ID', axis=1)\n",
    "#val_data = val_data.drop('ID', axis=1)\n",
    "#test_data = test_data.drop('ID', axis=1)\n",
    "\n",
    "print(f\"Train set size: {len(train_data)}\") \n",
    "print(f\"Validation set size: {len(val_data)}\") \n",
    "print(f\"Test set size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "      <th>dim3</th>\n",
       "      <th>dim4</th>\n",
       "      <th>dim5</th>\n",
       "      <th>dim6</th>\n",
       "      <th>dim7</th>\n",
       "      <th>dim8</th>\n",
       "      <th>dim9</th>\n",
       "      <th>...</th>\n",
       "      <th>dim23</th>\n",
       "      <th>dim24</th>\n",
       "      <th>dim25</th>\n",
       "      <th>dim26</th>\n",
       "      <th>dim27</th>\n",
       "      <th>dim28</th>\n",
       "      <th>dim29</th>\n",
       "      <th>dim30</th>\n",
       "      <th>dim31</th>\n",
       "      <th>dim32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-1000021</td>\n",
       "      <td>11.229898</td>\n",
       "      <td>29.822453</td>\n",
       "      <td>-4.026312</td>\n",
       "      <td>3.358199</td>\n",
       "      <td>0.231886</td>\n",
       "      <td>1.422046</td>\n",
       "      <td>-18.252083</td>\n",
       "      <td>12.020048</td>\n",
       "      <td>39.133064</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.145392</td>\n",
       "      <td>-5.654875</td>\n",
       "      <td>24.603487</td>\n",
       "      <td>25.662783</td>\n",
       "      <td>-27.461916</td>\n",
       "      <td>-21.059357</td>\n",
       "      <td>-16.466547</td>\n",
       "      <td>17.850504</td>\n",
       "      <td>-43.045430</td>\n",
       "      <td>-16.672508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-1000325</td>\n",
       "      <td>-31.725147</td>\n",
       "      <td>-0.395030</td>\n",
       "      <td>-0.939265</td>\n",
       "      <td>1.984952</td>\n",
       "      <td>47.066414</td>\n",
       "      <td>-37.844696</td>\n",
       "      <td>11.580789</td>\n",
       "      <td>8.266258</td>\n",
       "      <td>-1.891401</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.643673</td>\n",
       "      <td>-34.058014</td>\n",
       "      <td>20.732920</td>\n",
       "      <td>-26.947844</td>\n",
       "      <td>6.144578</td>\n",
       "      <td>13.846680</td>\n",
       "      <td>15.867406</td>\n",
       "      <td>-57.067223</td>\n",
       "      <td>-28.454120</td>\n",
       "      <td>-0.502026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-1000575</td>\n",
       "      <td>-13.318561</td>\n",
       "      <td>42.696262</td>\n",
       "      <td>-25.401484</td>\n",
       "      <td>0.247414</td>\n",
       "      <td>-8.130686</td>\n",
       "      <td>0.259071</td>\n",
       "      <td>15.793086</td>\n",
       "      <td>2.559356</td>\n",
       "      <td>-76.482475</td>\n",
       "      <td>...</td>\n",
       "      <td>3.452085</td>\n",
       "      <td>12.126092</td>\n",
       "      <td>-33.715843</td>\n",
       "      <td>-16.900974</td>\n",
       "      <td>-16.233204</td>\n",
       "      <td>-17.635584</td>\n",
       "      <td>-20.436428</td>\n",
       "      <td>-15.893841</td>\n",
       "      <td>-1.953091</td>\n",
       "      <td>-7.961510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID       dim1       dim2       dim3      dim4       dim5  \\\n",
       "0  sub-1000021  11.229898  29.822453  -4.026312  3.358199   0.231886   \n",
       "1  sub-1000325 -31.725147  -0.395030  -0.939265  1.984952  47.066414   \n",
       "2  sub-1000575 -13.318561  42.696262 -25.401484  0.247414  -8.130686   \n",
       "\n",
       "        dim6       dim7       dim8       dim9  ...      dim23      dim24  \\\n",
       "0   1.422046 -18.252083  12.020048  39.133064  ... -12.145392  -5.654875   \n",
       "1 -37.844696  11.580789   8.266258  -1.891401  ... -14.643673 -34.058014   \n",
       "2   0.259071  15.793086   2.559356 -76.482475  ...   3.452085  12.126092   \n",
       "\n",
       "       dim25      dim26      dim27      dim28      dim29      dim30  \\\n",
       "0  24.603487  25.662783 -27.461916 -21.059357 -16.466547  17.850504   \n",
       "1  20.732920 -26.947844   6.144578  13.846680  15.867406 -57.067223   \n",
       "2 -33.715843 -16.900974 -16.233204 -17.635584 -20.436428 -15.893841   \n",
       "\n",
       "       dim31      dim32  \n",
       "0 -43.045430 -16.672508  \n",
       "1 -28.454120  -0.502026  \n",
       "2  -1.953091  -7.961510  \n",
       "\n",
       "[3 rows x 33 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentTargetDataset(Dataset):\n",
    "    def __init__(self, latent_csv_path, target_npy_path, subjects_all_path, subject_list):\n",
    "        self.subject_list = subject_list\n",
    "\n",
    "        # Load latent vectors\n",
    "        latent_df = pd.read_csv(latent_csv_path)\n",
    "        latent_df = latent_df[latent_df[\"ID\"].isin(subject_list)]\n",
    "\n",
    "        # Enforce the order in subject_list\n",
    "        latent_df = latent_df.set_index(\"ID\").loc[subject_list].reset_index()\n",
    "        self.latents = latent_df.drop(columns=[\"ID\"])  # keep only the vector columns\n",
    "\n",
    "        # Load subject order from the .csv (matching the .npy)\n",
    "        subjects_all_df = pd.read_csv(subjects_all_path)\n",
    "\n",
    "        # Create mapping from subject ID to index in the .npy array\n",
    "        subject_to_index = {subj: idx for idx, subj in enumerate(subjects_all_df[\"Subject\"])}\n",
    "\n",
    "        # For each subject in subject_list, get its corresponding index in .npy file\n",
    "        try:\n",
    "            self.indices = [subject_to_index[subj] for subj in subject_list]\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Subject {e.args[0]} not found in subjects_all list\")\n",
    "\n",
    "        # Load target volumes\n",
    "        self.targets = np.load(target_npy_path)\n",
    "\n",
    "        # Safety check\n",
    "        assert len(self.latents) == len(self.indices), \"Mismatch between latent vectors and volume indices\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.latents)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        latent_vector = torch.tensor(self.latents.iloc[idx].values, dtype=torch.float32)\n",
    "        target_volume = torch.tensor(self.targets[self.indices[idx]], dtype=torch.float32).permute(3, 0, 1, 2)\n",
    "        return latent_vector, target_volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule_Learning(LightningDataModule):\n",
    "    def __init__(self, config, dataset_info):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dataset_info = dataset_info\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # Full paths to CSV and NPY files\n",
    "        train_path = os.path.join(self.config[\"model_to_decode_path\"], self.config[\"train_csv\"])\n",
    "        val_test_path = os.path.join(self.config[\"model_to_decode_path\"], self.config[\"val_test_csv\"])\n",
    "        print(self.config.keys())\n",
    "        subjects_all = os.path.join(self.config[\"dataset_folder\"], self.dataset_info['subjects_all'])\n",
    "        target_npy_path = os.path.join(self.config[\"dataset_folder\"], self.dataset_info['numpy_all'])\n",
    "\n",
    "        # Read splits\n",
    "        train_data = pd.read_csv(train_path)\n",
    "        val_test_data = pd.read_csv(val_test_path)\n",
    "        val_test_data['IID'] = val_test_data['ID'].apply(lambda x: int(x[4:]))\n",
    "\n",
    "        # Split into validation and test\n",
    "        train_subjects = train_data['ID'].tolist()\n",
    "        val_subjects = val_test_data[val_test_data['IID'] % 2 == 0]['ID'].tolist()\n",
    "        test_subjects = val_test_data[val_test_data['IID'] % 2 == 1]['ID'].tolist()\n",
    "\n",
    "        # Instantiate datasets\n",
    "        self.dataset_train = LatentTargetDataset(\n",
    "            latent_csv_path=train_path,\n",
    "            target_npy_path=target_npy_path,\n",
    "            subjects_all_path=subjects_all,\n",
    "            subject_list=train_subjects\n",
    "        )\n",
    "        self.dataset_val = LatentTargetDataset(\n",
    "            latent_csv_path=val_test_path,\n",
    "            target_npy_path=target_npy_path,\n",
    "            subjects_all_path=subjects_all,\n",
    "            subject_list=val_subjects\n",
    "        )\n",
    "        self.dataset_test = LatentTargetDataset(\n",
    "            latent_csv_path=val_test_path,\n",
    "            target_npy_path=target_npy_path,\n",
    "            subjects_all_path=subjects_all,\n",
    "            subject_list=test_subjects\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset_train,\n",
    "                          batch_size=self.config[\"batch_size\"],\n",
    "                          shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.dataset_val,\n",
    "                          batch_size=self.config[\"batch_size\"],\n",
    "                          shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.dataset_test,\n",
    "                          batch_size=self.config[\"batch_size\"],\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model_to_decode_path', 'dataset_folder', 'train_csv', 'val_test_csv', 'output_dir', 'batch_size', 'learning_rate', 'num_epochs', 'latent_dim', 'dropout', 'decoder_type', 'encoder_depth', 'block_depth', 'filters', 'last_kernel_size', 'activation'])\n",
      "torch.Size([32, 32]) torch.Size([32, 1, 18, 41, 38])\n"
     ]
    }
   ],
   "source": [
    "datamodule = DataModule_Learning(decoder_cfg, dataset_info)\n",
    "datamodule.setup()\n",
    "train_loader = datamodule.train_dataloader()\n",
    "\n",
    "for latent_vector, target_volume in train_loader:\n",
    "    print(latent_vector.shape, target_volume.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32]) torch.Size([32, 1, 16, 37, 37])\n"
     ]
    }
   ],
   "source": [
    "from dataloader import DataModule_Learning\n",
    "\n",
    "# Load configs\n",
    "decoder_cfg = OmegaConf.load(\"../configs/config.yaml\")\n",
    "encoder_config_path = os.path.join(decoder_cfg.model_to_decode_path, \".hydra\", \"config.yaml\")\n",
    "encoder_cfg = OmegaConf.load(encoder_config_path)\n",
    "encoder_cfg[\"dataset_folder\"] = decoder_cfg[\"dataset_folder\"]\n",
    "region = list(encoder_cfg.dataset.keys())[0]\n",
    "dataset_info = OmegaConf.to_container(encoder_cfg.dataset[region], resolve=True)\n",
    "\n",
    "# Instantiate and setup the datamodule\n",
    "dm = DataModule_Learning(decoder_cfg, dataset_info)\n",
    "dm.setup()\n",
    "\n",
    "# Get the data loaders\n",
    "train_loader = dm.train_dataloader()\n",
    "val_loader = dm.val_dataloader()\n",
    "test_loader = dm.test_dataloader()\n",
    "\n",
    "# Example loop\n",
    "for latent_vector, target_volume in train_loader:\n",
    "    print(latent_vector.shape, target_volume.shape)\n",
    "    break  # Just show one batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
